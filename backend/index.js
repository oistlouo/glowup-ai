const express = require('express');
const cors = require('cors');
const multer = require('multer');
const OpenAI = require('openai');
const cloudinary = require('cloudinary').v2;
const fs = require('fs');
const path = require('path');
require('dotenv').config();

const app = express();

app.use(cors({
  origin: ['https://glowup-ai.vercel.app'],
}));

app.use(express.json());
const storage = multer.memoryStorage();
const upload = multer({ storage });

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

cloudinary.config({
  cloud_name: process.env.CLOUDINARY_CLOUD_NAME,
  api_key: process.env.CLOUDINARY_API_KEY,
  api_secret: process.env.CLOUDINARY_API_SECRET,
});

app.post('/analyze', upload.single('image'), async (req, res) => {
  try {
    const file = req.file;
    if (!file) return res.status(400).json({ error: 'No image uploaded.' });

    const streamUpload = () => new Promise((resolve, reject) => {
      const uploadStream = cloudinary.uploader.upload_stream(
        {
          resource_type: 'image',
          folder: 'glowup-ai',
          use_filename: false,
          unique_filename: true,
          overwrite: false,
        },
        (error, result) => {
          if (error) reject(error);
          else resolve(result);
        }
      );
      uploadStream.end(file.buffer);
    });

    const uploaded = await streamUpload();
    const imageUrl = uploaded.secure_url;
    console.log("âœ… Uploaded Image URL:", imageUrl);

    const promptPath = path.join(__dirname, 'templates', 'prompt_ko.txt');
    if (!fs.existsSync(promptPath)) {
      throw new Error(`í”„ë¡¬í”„íŠ¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: ${promptPath}`);
    }

    const rawPrompt = fs.readFileSync(promptPath, 'utf8');
    const name = req.body.name || 'ì‚¬ìš©ì';
    const age = req.body.age || '??';
    const prompt = rawPrompt.replace(/\[name\]/g, name).replace(/\[age\]/g, age);

    let attempts = 0;
    let rawResult = '';
    let isComplete = false;

    while (attempts < 3 && !isComplete) {
      attempts++;
      const completion = await openai.chat.completions.create({
        model: 'gpt-4-turbo',
        messages: [
          {
            role: 'user',
            content: [
              { type: 'text', text: prompt },
              { type: 'image_url', image_url: { url: imageUrl } },
            ],
          },
        ],
        stream: false,
        temperature: 0.7,
        max_tokens: 3600,
      });

      rawResult = completion.choices?.[0]?.message?.content || '';

      rawResult = rawResult
        .replace(/```(json|html)?[\s\S]*?```/g, '')
        .replace(/^```html/, '')
        .replace(/JSON Output:/g, '')
        .trim();

      isComplete =
        rawResult.includes('<h1>ğŸŒ¿ ì¢…í•© í”¼ë¶€ ë¶„ì„ ë¦¬í¬íŠ¸</h1>') &&
        !rawResult.toLowerCase().includes('this html format') &&
        !rawResult.toLowerCase().includes("i'm sorry") &&
        !rawResult.toLowerCase().includes('no analysis');
    }

    // ëˆ„ë½ëœ í•­ëª© ë³´ì™„ ì‚½ì…
    const requiredSections = [
      '1. í”¼ë¶€ ë‚˜ì´', '2. í”¼ì§€', '3. ìˆ˜ë¶„ ìƒíƒœ', '4. í”¼ë¶€ê²°',
      '5. ìƒ‰ì†Œì¹¨ì°©', '6. ëª¨ê³µ ê°€ì‹œì„±', '7. ë¯¼ê°ë„',
      '8. ì£¼ë¦„', '9. í”¼ë¶€ í†¤', '10. ì—¬ë“œë¦„'
    ];

    requiredSections.forEach((section) => {
      if (!rawResult.includes(`ğŸ”¹ ${section}`)) {
        rawResult += `\n<h2>ğŸ”¹ ${section}</h2>\n<div class="card" style="background:#2a2a2a;color:#fff;border-radius:12px;padding:20px;margin-bottom:20px">\n<p><strong>ì ìˆ˜:</strong> 5/10</p>\n<p><strong>ì§„ë‹¨ ê²°ê³¼:</strong> í•´ë‹¹ ë¶€ìœ„ê°€ ì´ë¯¸ì§€ì—ì„œ ëª…í™•íˆ ë³´ì´ì§€ ì•Šì•„ ì •ë°€ ë¶„ì„ì€ ì–´ë ¤ì› ìŠµë‹ˆë‹¤. ê´€ì°° ê°€ëŠ¥í•œ ë²”ìœ„ì—ì„œ ì œí•œì ìœ¼ë¡œ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.</p>\n<p><strong>ì¶”ì²œ ì†”ë£¨ì…˜:</strong> ì¼ë°˜ì ì¸ ê´€ë¦¬ ê¸°ì¤€ì— ë”°ë¼ ê¸°ì´ˆì ì¸ ìŠ¤í‚¨ì¼€ì–´ ë£¨í‹´ì„ ìœ ì§€í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.</p>\n<p><strong>ì¶”ì²œ ì œí’ˆ:</strong> La Roche-Posay Toleriane Cream</p>\n<p><strong>ì¶”ì²œ ì´ìœ :</strong> ìê·¹ì´ ì ê³  ë¯¼ê°í•œ í”¼ë¶€ì—ë„ ì í•©í•œ ë³´ìŠµ ë° ì§„ì • íš¨ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.</p>\n</div>`;
      }
    });

    res.json({
      fullHtml: rawResult,
      imageUrl,
      previewInsights: [],
    });
  } catch (err) {
    console.error('âŒ Server error:', err);
    if (err.response) {
      const text = await err.response.text?.();
      console.error('ğŸ” OpenAI API Error Response:', text);
    }
    res.status(500).json({ error: 'Internal server error.' });
  }
});

const PORT = process.env.PORT || 5001;
app.listen(PORT, () => {
  console.log(`âœ… Backend running on http://localhost:${PORT}`);
});